import { fal } from "@fal-ai/client";
import { GamePart, BBox, pixelToRel } from '../types';

const configureFal = () => {
  const apiKey = process.env.FAL_KEY;
  if (!apiKey) throw new Error("FAL_KEY not found in environment");
  fal.config({ credentials: apiKey });
};

export const generateAssetArt = async (
  originalImageBase64: string,
  layoutTemplateBase64: string,
  parts: GamePart[],
  stylePrompt: string = "Detailed, clean 2D game art style, vibrant colors"
): Promise<string> => {
  configureFal();

  // Create number-to-part mapping (matches numbers drawn on atlas)
  const mapping = parts.map((p, index) => {
    const num = index + 1;
    const target = p.atlasRect!;
    const bboxStr = `[${p.bbox.map(v => v.toFixed(3)).join(',')}]`;
    const shapeDesc = p.shape.type === 'path' ? 'path' : p.shape.type;
    return `- Box #${num} = "${p.name}": (Shape: ${shapeDesc}, BBox: ${bboxStr}) -> (Target rect: [${target.x},${target.y},${target.w},${target.h}])`;
  }).join("\n");

  const fullPrompt = `
TASK: separate the original image into parts.

INPUTS:
1. ORIGINAL_IMAGE: The original object with all parts anotated.
2. LAYOUT_TEMPLATE: A square image with numbered boxes (1, 2, 3...) where you must draw the parts.

NUMBER-TO-PART MAPPING:
${mapping}

INSTRUCTIONS:
- Each numbered box in the LAYOUT_TEMPLATE corresponds to a part from the ORIGINAL_IMAGE.
- Fill each numbered box with the corresponding part from the ORIGINAL_IMAGE without changing the perspective or style.
- DO NOT include the dashed bounding box lines or numbers in your output.
- OUTPUT MUST ONLY BE THE DRAWN PARTS on a clean white background.
- OCCLUSION HANDLING: For parts partially hidden in the original image, generate the ENTIRE part as it would look if fully visible and detached.
- Keep lighting, style, and perspective consistent across all parts.
  `.trim();

  const originalImageDataUrl = `data:image/png;base64,${originalImageBase64}`;
  const layoutTemplateDataUrl = `data:image/png;base64,${layoutTemplateBase64}`;

  const result = await fal.subscribe("fal-ai/flux-2-pro/edit", {
    input: {
      prompt: fullPrompt,
      image_size: "auto",
      safety_tolerance: "2",
      enable_safety_checker: true,
      output_format: "png",
      image_urls: [originalImageDataUrl, layoutTemplateDataUrl]
    },
    logs: true,
    onQueueUpdate: (update) => {
      if (update.status === "IN_PROGRESS" && update.logs) {
        update.logs.map((log) => log.message).forEach(console.log);
      }
    },
  });

  const data = result.data as { images?: Array<{ url: string }> };

  if (data.images && data.images.length > 0) {
    const imageUrl = data.images[0].url;

    const response = await fetch(imageUrl);
    const blob = await response.blob();

    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        const base64 = (reader.result as string).split(',')[1];
        resolve(base64);
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  throw new Error("No image generated by fal.ai");
};

export const segmentPartSAM3 = async (
  imageUrl: string,
  bbox: BBox, // relative 0-1
  prompt: string = "part"
): Promise<string> => {
  configureFal();

  // Convert bbox to box_prompts format [x_min, y_min, x_max, y_max]
  // SAM3 expects pixel coordinates? Or relative?
  // Documentation says: "Box prompt coordinates (x_min, y_min, x_max, y_max)"
  // Usually this is pixel coords for SAM unless specified otherwise.
  // The input image is 1024x1024 typically.
  // We should pass 1024 scaled coords.

  const size = 1024;
  const box_prompts = [
    {
      x_min: Math.round(bbox[0] * size),
      y_min: Math.round(bbox[1] * size),
      x_max: Math.round(bbox[2] * size),
      y_max: Math.round(bbox[3] * size),
      object_id: 1
    }
  ];

  // Create a blob from the base64 string
  const byteCharacters = atob(imageUrl);
  const byteNumbers = new Array(byteCharacters.length);
  for (let i = 0; i < byteCharacters.length; i++) {
    byteNumbers[i] = byteCharacters.charCodeAt(i);
  }
  const byteArray = new Uint8Array(byteNumbers);
  const blob = new Blob([byteArray], { type: 'image/png' });

  // Upload the image to Fal storage
  const uploadedUrl = await fal.storage.upload(blob);

  const result = await fal.subscribe("fal-ai/sam-3/image", {
    input: {
      image_url: uploadedUrl,
      prompt: prompt,
      box_prompts: box_prompts as any,
      apply_mask: true,
      output_format: "png",
      return_multiple_masks: false
    },
    logs: true,
    onQueueUpdate: (update) => {
      if (update.status === "IN_PROGRESS" && update.logs) {
        update.logs.map((log) => log.message).forEach(console.log);
      }
    },
  });

  const data = result.data as { image?: { url: string }, masks?: Array<{ url: string }> };

  // Checking output schema: "image" (Primary segmented mask preview) or "masks" (list of images)
  const maskUrl = data.image?.url || data.masks?.[0]?.url;

  if (maskUrl) {
    const response = await fetch(maskUrl);
    const blob = await response.blob();
    return new Promise((resolve, reject) => {
      const reader = new FileReader();
      reader.onloadend = () => {
        const base64 = (reader.result as string).split(',')[1];
        resolve(base64);
      };
      reader.onerror = reject;
      reader.readAsDataURL(blob);
    });
  }

  throw new Error("No mask generated by SAM3");
};
